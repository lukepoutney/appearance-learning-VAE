{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB Image Reconstruction VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code used is currently heavily based on a tutorial getting started with variational autoencoder using pytorch on Debugger Cafe  \n",
    "This will be changed over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__() \n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        #self.dropout = nn.Dropout2d(p=0.1) # need to call in forward but turn off automatically in eval()\n",
    "        \n",
    "        self.mu = nn.Linear(init_kernel*191*191, z_dim)\n",
    "        self.sigma = nn.Linear(init_kernel*191*191, z_dim)\n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"before anything\")\n",
    "        #print(x.shape)\n",
    "        x = self.enc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.enc4(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.enc5(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #print(\"after flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x) # fix!!!!name\n",
    "        \n",
    "        #return mu, log_var #fix!!! name\n",
    "        ## get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        return z, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(z_dim, init_kernel*191*191)\n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.lin1(z)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_kernel, 191, 191)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.dec1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        sample, z_mu, z_logvar = self.encoder(x)\n",
    "        \n",
    "        #z_mu, z_logvar = self.encoder(x)\n",
    "        \n",
    "        #std = torch.exp(z_logvar)\n",
    "        #eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        #sample = z_mu + (eps * std) # sampling\n",
    "        \n",
    "        reconstruction = self.decoder(sample)\n",
    "        \n",
    "        return reconstruction, z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        #print(data)\n",
    "        data, _ = data\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, _ = data\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(batch_size, 3, 200, 200)[:8], \n",
    "                                  reconstruction.view(batch_size, 3, 200, 200)[:8]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters (change in optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# leanring parameters\n",
    "\n",
    "#Number of conv layers: 3,4,5\n",
    "\n",
    "epochs = 25 # 50,100\n",
    "batch_size = 8 #8,16,32,64?\n",
    "lr = 0.0001 #0.002, 0.003\n",
    "\n",
    "kernel_size = 4 #3,4,5,6?\n",
    "stride = 1 #1,2,3,4?\n",
    "padding = 0 #0,1,2,3,4?\n",
    "init_kernel = 8 #4,8,16,32 #initial number of filters\n",
    "\n",
    "latent_dim = 96\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#Add remove layers ***make priority!!!!\n",
    "#Optimizer (setup just a few cells below)\n",
    "\n",
    "#Do dropout (across multiple layers, with multiple p values?)\n",
    "#Do pooling? Max pool or average pool?\n",
    "#Do activation function #There's a lot\n",
    "#Batch normalization #Looked it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ActiveVisionDataset(csv_file='imgs/rgbCSV.csv', root_dir= 'imgs/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [2400, 600])\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "its runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "batch_sizes = [8]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "'''\n",
    "dataset = ActiveVisionDataset(csv_file='imgs/rgbCSV.csv', root_dir= 'imgs/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [2400, 600])\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = ConvVAE(latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "parameter = 'testertsne'\n",
    "value = 0\n",
    "os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "#parameters = ['layers']\n",
    "#values = ['3,4,5,6,7']\n",
    "\n",
    "#for parameter in parameters:\n",
    "    #for value in values:\n",
    "\n",
    "        #Create a folder here\n",
    "        #os.makedirs(parameter+value, exist_ok=True)\n",
    "value = str(value)\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = fit(model, train_loader)\n",
    "    val_epoch_loss = validate(model, val_loader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "torch.save(model.state_dict(), filepath)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yticks(np.arange(int(math.floor(min(train_loss) / 100.0)) * 100, max(train_loss)+1, 1000))\n",
    "plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "\n",
    "with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "    wr.writerows(zip(train_loss, val_loss))\n",
    "\n",
    "with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([parameter, value ,train_loss[-1], val_loss[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yticks(np.arange(int(math.floor(min(train_loss) / 100.0)) * 100, max(train_loss)+1, 1000))\n",
    "plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "    wr.writerows(zip(train_loss, val_loss))\n",
    "\n",
    "with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([parameter, value ,train_loss[-1], val_loss[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is test set time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which model am i tryna load in huh\n",
    "PATH = os.path.join(os.getcwd(), \"outputs\", \"batch_sizes8\", \"batch_sizes8.pth\")\n",
    "\n",
    "ConvVAE = ConvVAE(latent_dim)#.to(device)\n",
    "ConvVAE.load_state_dict(torch.load(PATH))\n",
    "ConvVAE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So what imma do is call something similar to\n",
    "#validate to gather a load of things and ill store the reslts\n",
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, label = data\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            z, mu, logvar = model.encoder(data.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make this a test set!!!\n",
    "test_latent_var, test_target = generate_latent_vectors(ConvVAE, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(test_latent_var))\n",
    "#print(len(test_target))\n",
    "#print(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib qt\n",
    "from IPython import display\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.array(test_latent_var)\n",
    "target = np.array(test_target)\n",
    "tsne = TSNE(n_components=2, init=\"pca\", random_state=0)\n",
    "\n",
    "X = tsne.fit_transform(latent)\n",
    "\n",
    "data = np.vstack((X.T, target)).T\n",
    "df = pd.DataFrame(data=data, columns=[\"z1\", \"z2\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(str)\n",
    "\n",
    "fig = px.scatter(df, x=\"z1\", y=\"z2\", color=\"label\")\n",
    "\n",
    "pio.write_html(fig, file=\"raw.html\", auto_open=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adding/removing layers: CNN layers and fully connected layers  \n",
    "2. Varying batch sizes  \n",
    "3. Varying numbers of training epochs  \n",
    "4. Applying different levels of drop-out  \n",
    "5. Applying different types of pooling  \n",
    "6. Applying different types of activation function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data in train_loader:\n",
    "#    print(im_path)\n",
    "#    print(\"Data: \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n",
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "#print(list(train_data))\n",
    "#type(train_loader)\n",
    "#print(train_data.shape)\n",
    "\n",
    "for i, (images, labels, coords) in enumerate(train_loader):\n",
    "    print(coords.shape)\n",
    "#    print(labels.shape)\n",
    "    #print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    vars()['bruh'+str(i)] = i\n",
    "\n",
    "print (bruh0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i in range(2,6):\n",
    "            vars()[self.'enc'+str(i)] = nn.Conv2d(\n",
    "                in_channels=init_kernel, out_channels=init_kernel*i, kernel_size=kernel_size, \n",
    "                stride=stride, padding=padding\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        outputs = ConvVAE.encoder(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(outputs)\n",
    "        print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
