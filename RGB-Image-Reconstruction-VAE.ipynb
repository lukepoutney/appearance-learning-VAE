{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB Image Reconstruction VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code used is currently heavily based on a tutorial getting started with variational autoencoder using pytorch on Debugger Cafe  \n",
    "This will be changed over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        #cam_loc = torch.tensor(int(self.annotations.iloc[index,2]))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__() \n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        #self.dropout = nn.Dropout2d(p=0.1) # need to call in forward but turn off automatically in eval()\n",
    "        \n",
    "        self.mu = nn.Linear(init_kernel*191*191, z_dim)\n",
    "        self.sigma = nn.Linear(init_kernel*191*191, z_dim)\n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"before anything\")\n",
    "        #print(x.shape)\n",
    "        x = self.enc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.enc4(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.enc5(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #print(\"after flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x) # fix!!!!name\n",
    "        \n",
    "        #return mu, log_var #fix!!! name\n",
    "        ## get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(z_dim, init_kernel*191*191)\n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.lin1(z)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_kernel, 191, 191)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.dec1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        sample = self.encoder(x)\n",
    "        #z_mu, z_logvar = self.encoder(x)\n",
    "        \n",
    "        #std = torch.exp(z_logvar)\n",
    "        #eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        #sample = z_mu + (eps * std) # sampling\n",
    "        \n",
    "        reconstruction = self.decoder(sample)\n",
    "        \n",
    "        return reconstruction, z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        #print(data)\n",
    "        data, _ = data\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, _ = data\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(batch_size, 3, 200, 200)[:8], \n",
    "                                  reconstruction.view(batch_size, 3, 200, 200)[:8]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters (change in optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "\n",
    "#Number of conv layers: 3,4,5\n",
    "\n",
    "epochs = 50 # 50,100\n",
    "batch_size = 8 #8,16,32,64?\n",
    "lr = 0.0001 #0.002, 0.003\n",
    "\n",
    "kernel_size = 4 #3,4,5,6?\n",
    "stride = 1 #1,2,3,4?\n",
    "padding = 0 #0,1,2,3,4?\n",
    "init_kernel = 8 #4,8,16,32 #initial number of filters\n",
    "\n",
    "latent_dim = 96\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#Add remove layers ***make priority!!!!\n",
    "#Optimizer (setup just a few cells below)\n",
    "\n",
    "#Do dropout (across multiple layers, with multiple p values?)\n",
    "#Do pooling? Max pool or average pool?\n",
    "#Do activation function #There's a lot\n",
    "#Batch normalization #Looked it up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ActiveVisionDataset(csv_file='imgs/rgbCSV.csv', root_dir= 'imgs/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [2400, 600])\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbsjhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_sizes = [8]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    \n",
    "    dataset = ActiveVisionDataset(csv_file='imgs/rgbCSV.csv', root_dir= 'imgs/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "    train_data, val_data = torch.utils.data.random_split(dataset, [2400, 600])\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = ConvVAE(latent_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "    parameter = 'batch_sizes'\n",
    "    value = batch_size\n",
    "    os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "    os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "    #parameters = ['layers']\n",
    "    #values = ['3,4,5,6,7']\n",
    "\n",
    "    #for parameter in parameters:\n",
    "        #for value in values:\n",
    "\n",
    "            #Create a folder here\n",
    "            #os.makedirs(parameter+value, exist_ok=True)\n",
    "    value = str(value)\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        train_epoch_loss = fit(model, train_loader)\n",
    "        val_epoch_loss = validate(model, val_loader)\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        val_loss.append(val_epoch_loss)\n",
    "        print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "    filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "    plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.yticks(np.arange(int(math.floor(min(train_loss) / 100.0)) * 100, max(train_loss)+1, 1000))\n",
    "    plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "    \n",
    "    with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "        wr.writerows(zip(train_loss, val_loss))\n",
    "\n",
    "    with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerow([parameter, value ,train_loss[-1], val_loss[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok boomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yticks(np.arange(int(math.floor(min(train_loss) / 100.0)) * 100, max(train_loss)+1, 1000))\n",
    "plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "    wr.writerows(zip(train_loss, val_loss))\n",
    "\n",
    "with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([parameter, value ,train_loss[-1], val_loss[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C1ASSIFICATIONNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Shape 0', 'Shape 1', 'Shape 2', 'Shape 3', 'Shape 4', 'Shape 5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = ['Shape 0', 'Shape 1', 'Shape 2', 'Shape 3', 'Shape 4', 'Shape 5']\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(8)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which model am i tryna load in huh\n",
    "PATH = os.path.join(os.getcwd(), \"outputs\", \"batch_sizes8\", \"batch_sizes8.pth\")\n",
    "\n",
    "ConvVAE = ConvVAE(latent_dim)#.to(device)\n",
    "ConvVAE.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvVAE.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should probably be a test set here!\n",
    "\n",
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, _ = data\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            z = ConvVAE.encoder(data)\n",
    "            latent.append(z)\n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_latent_var = generate_latent_vectors(ConvVAE, train_loader)\n",
    "test_latent_var = generate_latent_vectors(ConvVAE, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(len(latent_var))\n",
    "train_latent_var = latent_var[0:60]\n",
    "test_latent_var = latent_var[60:75]\n",
    "print(len(train_latent_var))\n",
    "print(len(test_latent_var))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "        #self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(96, 60)\n",
    "        #self.fc2 = nn.Linear(60, 30)\n",
    "        self.fc3 = nn.Linear(60, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.pool(F.relu(self.conv1(x)))\n",
    "        #x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        _, labels = data\n",
    "        z = train_latent_var[i]\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #if torch.cuda.is_available():\n",
    "        #    z = z.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(z)#.to(device)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(val_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,8):\n",
    "    outputs = net(test_latent_var[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "prediction = []\n",
    "actual = []\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(test_latent_var[i])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        prediction.append(predicted.tolist())\n",
    "        actual.append(labels.tolist())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        i+=1\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction\")\n",
    "print(prediction)\n",
    "\n",
    "print(\"actual\")\n",
    "print(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adding/removing layers: CNN layers and fully connected layers  \n",
    "2. Varying batch sizes  \n",
    "3. Varying numbers of training epochs  \n",
    "4. Applying different levels of drop-out  \n",
    "5. Applying different types of pooling  \n",
    "6. Applying different types of activation function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for data in train_loader:\n",
    "#    print(im_path)\n",
    "#    print(\"Data: \", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(train_data))\n",
    "#type(train_loader)\n",
    "#print(train_data.shape)\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(images.shape)\n",
    "#    print(labels.shape)\n",
    "#    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    vars()['bruh'+str(i)] = i\n",
    "\n",
    "print (bruh0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i in range(2,6):\n",
    "            vars()[self.'enc'+str(i)] = nn.Conv2d(\n",
    "                in_channels=init_kernel, out_channels=init_kernel*i, kernel_size=kernel_size, \n",
    "                stride=stride, padding=padding\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        outputs = ConvVAE.encoder(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(outputs)\n",
    "        print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
