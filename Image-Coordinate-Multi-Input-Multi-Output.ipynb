{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__() \n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        #self.dropout = nn.Dropout2d(p=0.1) # need to call in forward but turn off automatically in eval()\n",
    "        \n",
    "        self.mu = nn.Linear(init_kernel*191*191, z_dim)\n",
    "        self.sigma = nn.Linear(init_kernel*191*191, z_dim)\n",
    "        \n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"before anything\")\n",
    "        #print(x.shape)\n",
    "        x = self.enc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.enc3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.enc4(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.enc5(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #print(\"after flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x) # fix!!!!name\n",
    "        \n",
    "        #return mu, log_var #fix!!! name\n",
    "        ## get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        return z, mu, log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(z_dim, init_kernel*191*191)\n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.lin1(z)\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(-1, init_kernel, 191, 191)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.dec1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        sample, z_mu, z_logvar = self.encoder(x)\n",
    "        #z_mu, z_logvar = self.encoder(x)\n",
    "        \n",
    "        #std = torch.exp(z_logvar)\n",
    "        #eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        #sample = z_mu + (eps * std) # sampling\n",
    "        \n",
    "        reconstruction = self.decoder(sample)\n",
    "        \n",
    "        return reconstruction, z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below should be the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25 # 50,100\n",
    "batch_size = 1 #8,16,32,64?\n",
    "lr = 0.0001 #0.002, 0.003\n",
    "\n",
    "kernel_size = 4 #3,4,5,6?\n",
    "stride = 1 #1,2,3,4?\n",
    "padding = 0 #0,1,2,3,4?\n",
    "init_kernel = 8 #4,8,16,32 #initial number of filters\n",
    "\n",
    "latent_dim = 96\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ActiveVisionDataset(csv_file='imgs/rgbCSV.csv', root_dir= 'imgs/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [2400, 600])\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which model am i tryna load in huh\n",
    "PATH = os.path.join(os.getcwd(), \"outputs\", \"batch_sizes8\", \"batch_sizes8.pth\")\n",
    "\n",
    "ConvVAE = ConvVAE(latent_dim)#.to(device)\n",
    "ConvVAE.load_state_dict(torch.load(PATH))\n",
    "ConvVAE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvVAE.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, labels = data\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            z, mu, logvar = model.encoder(data.cuda())\n",
    "            latent.extend(mu)\n",
    "            target.extend(labels)\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Maybe don't need this ngl or generate above?\n",
    "train_latent_var, train_target = generate_latent_vectors(ConvVAE, train_loader)\n",
    "test_latent_var, test_target = generate_latent_vectors(ConvVAE, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombNet(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(CombNet, self).__init__()\n",
    "        \n",
    "        #Combined fully conntected layer\n",
    "        #Add this playing around stage later innit\n",
    "        \n",
    "        \n",
    "        #Img decoding\n",
    "        '''\n",
    "        self.lin1 = nn.Linear(z_dim, init_kernel*191*191)\n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        '''\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        \n",
    "        #Polar Coord decoding\n",
    "        self.coord1 = nn.linear(z_dim, 50)\n",
    "        \n",
    "        self.coord2 = nn.linear(50,10)\n",
    "        \n",
    "        self.coord3 = nn.linear(10,3)\n",
    "        \n",
    "    \n",
    "    def forward(self,img,coord):\n",
    "        \n",
    "        img, mu, log_var = ConvVAE.encoder(img)\n",
    "        \n",
    "        #reshape in some way, maybe expansion?\n",
    "        x = torch.cat((img, coord), dim=1) #Maybe 0!!!!!!!!!!\n",
    "        x = self.linear1(z_dim+3, 150)\n",
    "        x = self.linear2(150,300)\n",
    "        x = self.linear3(300,z_dim) #Doesn't have to go back to z_dim I was just feeling it \n",
    "\n",
    "        ##feed new latent vector into deocders that try to get the original things\n",
    "        \n",
    "        #IMG time\n",
    "        #------------------------------------------------\n",
    "        img = self.lin1(x)\n",
    "        img=F.relu(img)\n",
    "        \n",
    "        img=img.view(-1, init_kernel, 191, 191)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.dec1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec2(x)\n",
    "        #x = F.relu(x)\n",
    "        img = self.dec3(img)\n",
    "        img = F.relu(img)\n",
    "        img = self.dec4(img)\n",
    "        img = F.relu(img)\n",
    "        img = self.dec5(img)\n",
    "        reconstruction_img = torch.sigmoid(img)\n",
    "        #------------------------------------------------\n",
    "        \n",
    "        #Coord Time\n",
    "        #------------------------------------------------\n",
    "        coord = self.coordlin1(coord)\n",
    "        coord = F.relu(coord)\n",
    "        coord = self.coordlin2(coord)\n",
    "        coord = F.relu(coord)\n",
    "        coord = self.coordlin3(coord)\n",
    "        reconstruction_coord = torch.sigmoid(coord)\n",
    "        \n",
    "        #------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # decode reshaped vector into full image\n",
    "        # decode reshaped vector into polar coords\n",
    "        \n",
    "        return reconstruction_img, reconstruction_coord\n",
    "        \n",
    "#How to do loss?\n",
    "# % loss per epoch?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copied straight from original. Need to mess with them a bit to make work for this!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate loss:\n",
    "#Get difference between original and reconstructed img and coord\n",
    "#Add kldivergence of encoder VAE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        #print(data)\n",
    "        data, _ = data\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        loss = final_loss(bce_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, _ = data\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            loss = final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(batch_size, 3, 200, 200)[:8], \n",
    "                                  reconstruction.view(batch_size, 3, 200, 200)[:8]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Parameters for NEW model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "\n",
    "#Number of conv layers: 3,4,5\n",
    "\n",
    "epochs = 25 # 50,100\n",
    "batch_size = 8 #8,16,32,64?\n",
    "lr = 0.0001 #0.002, 0.003\n",
    "\n",
    "kernel_size = 4 #3,4,5,6?\n",
    "stride = 1 #1,2,3,4?\n",
    "padding = 0 #0,1,2,3,4?\n",
    "init_kernel = 8 #4,8,16,32 #initial number of filters\n",
    "\n",
    "latent_dim = 96\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#Add remove layers ***make priority!!!!\n",
    "#Optimizer (setup just a few cells below)\n",
    "\n",
    "#Do dropout (across multiple layers, with multiple p values?)\n",
    "#Do pooling? Max pool or average pool?\n",
    "#Do activation function #There's a lot\n",
    "#Batch normalization #Looked it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
