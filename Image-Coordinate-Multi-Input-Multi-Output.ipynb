{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "#import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "#import model\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Encoder, self).__init__() \n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        #self.bn_enc1 = nn.BatchNorm2d(init_kernel)\n",
    "        #self.bn_enc2 = nn.BatchNorm2d(init_kernel*2)\n",
    "        #self.bn_enc3 = nn.BatchNorm2d(init_kernel)\n",
    "        \n",
    "        self.lin1 = nn.Linear(init_kernel*119*119, 1000)\n",
    "        self.lin2 = nn.Linear(1000, 500)\n",
    "        self.lin3 = nn.Linear(500,250)\n",
    "        self.mu = nn.Linear(250, z_dim)\n",
    "        self.sigma = nn.Linear(250, z_dim)\n",
    "        \n",
    "        self.bn_lin1 = nn.BatchNorm1d(1000)\n",
    "        self.bn_lin2 = nn.BatchNorm1d(500)\n",
    "        self.bn_lin3 = nn.BatchNorm1d(250)\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"before anything\")\n",
    "        #print(x.shape)\n",
    "        x = self.enc1(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn_enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn_enc2(x)\n",
    "        x = self.enc3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn_enc3(x)\n",
    "        #x = self.enc4(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.enc5(x)\n",
    "        #x = F.relu(x)\n",
    "        \n",
    "        #print(\"before flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        #print(\"after flatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn_lin1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn_lin2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn_lin3(x)\n",
    "        # get `mu` and `log_var`\n",
    "        mu = self.mu(x)\n",
    "        log_var = self.sigma(x)\n",
    "        ## get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        return z, mu, log_var\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(z_dim, 250)\n",
    "        self.lin2 = nn.Linear(250, 500)        \n",
    "        self.lin3 = nn.Linear(500, 1000)\n",
    "        self.lin4 = nn.Linear(1000, init_kernel*119*119)\n",
    "        \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        #self.bn_enc1 = nn.BatchNorm2d(init_kernel*2)\n",
    "        #self.bn_enc2 = nn.BatchNorm2d(init_kernel)\n",
    "        \n",
    "        self.bn_lin1 = nn.BatchNorm1d(250)\n",
    "        self.bn_lin2 = nn.BatchNorm1d(500)\n",
    "        self.bn_lin3 = nn.BatchNorm1d(1000)\n",
    "        self.bn_lin4 = nn.BatchNorm1d(init_kernel*119*119)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.lin1(z)\n",
    "        x=F.relu(x)\n",
    "        x = self.bn_lin1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        x=F.relu(x)\n",
    "        x = self.bn_lin2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin3(x)\n",
    "        x=F.relu(x)\n",
    "        x = self.bn_lin3(x)\n",
    "        x = self.lin4(x)\n",
    "        x=F.relu(x)\n",
    "        x = self.bn_lin4(x)\n",
    "        \n",
    "        x=x.view(-1, init_kernel, 119, 119)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.dec1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec2(x)\n",
    "        #x = F.relu(x)\n",
    "        x = self.dec3(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn_enc1(x)\n",
    "        x = self.dec4(x)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn_enc2(x)\n",
    "        x = self.dec5(x)\n",
    "        reconstruction = torch.sigmoid(x)\n",
    "        \n",
    "        return reconstruction\n",
    "    \n",
    "    \n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.encoder = Encoder(z_dim)\n",
    "        self.decoder = Decoder(z_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        sample, z_mu, z_logvar = self.encoder(x)\n",
    "        \n",
    "        #z_mu, z_logvar = self.encoder(x)\n",
    "        \n",
    "        #std = torch.exp(z_logvar)\n",
    "        #eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        #sample = z_mu + (eps * std) # sampling\n",
    "        \n",
    "        reconstruction = self.decoder(sample)\n",
    "        \n",
    "        return reconstruction, z_mu, z_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below should be the optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leanring parameters\n",
    "\n",
    "#Number of conv layers: 3,4,5\n",
    "\n",
    "epochs = 50 # 50,100\n",
    "batch_size = 16 #8,16,32,64?\n",
    "lr = 0.001 #0.002, 0.003\n",
    "\n",
    "kernel_size = 4 #3,4,5,6?\n",
    "stride = 1 #1,2,3,4?\n",
    "padding = 0 #0,1,2,3,4?\n",
    "init_kernel = 8 #4,8,16,32 #initial number of filters\n",
    "\n",
    "latent_dim = 96\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActiveVisionDataset (Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, index_col=None)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        if type(index) == torch.Tensor:\n",
    "            index = index.item()\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = io.imread(img_path)\n",
    "        shape_label = torch.tensor(int(self.annotations.iloc[index,1]))\n",
    "        cam_loc = torch.tensor(ast.literal_eval(self.annotations.iloc[index,2]))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, shape_label, cam_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ActiveVisionDataset(csv_file='imgs/TrainSet/rgbCSV.csv', root_dir= 'imgs/TrainSet/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "val_data = ActiveVisionDataset(csv_file='imgs/ValSet/rgbCSV.csv', root_dir= 'imgs/ValSet/rgbImg/', transform = torchvision.transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Which model am i tryna load in huh\n",
    "PATH = os.path.join(os.getcwd(), \"outputs\", \"4oT800V200BatchNorm64DropoutEpochs500\", \"4oT800V200BatchNorm64DropoutEpochs500.pth\")\n",
    "\n",
    "ConvVAE = ConvVAE(latent_dim)#.to(device)\n",
    "ConvVAE.load_state_dict(torch.load(PATH))\n",
    "ConvVAE.to(device)\n",
    "ConvVAE.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvVAE.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_latent_vectors(model, dataloader):\n",
    "#     model.eval()\n",
    "#     latent = []\n",
    "#     target = []\n",
    "#     with torch.no_grad():\n",
    "#         for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "#             data, labels = data\n",
    "#             #if torch.cuda.is_available():\n",
    "#             #    data = data.to(device)\n",
    "#             z, mu, logvar = model.encoder(data.cuda())\n",
    "#             latent.extend(mu)\n",
    "#             target.extend(labels)\n",
    "#         return latent, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train_latent_var, train_target = generate_latent_vectors(ConvVAE, train_loader)\n",
    "#test_latent_var, test_target = generate_latent_vectors(ConvVAE, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombNet(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(CombNet, self).__init__()\n",
    "        \n",
    "        #Combined fully conntected layer\n",
    "                \n",
    "        self.linear1 = nn.Linear(z_dim+3, 150)\n",
    "        self.linear2 = nn.Linear(150,300)\n",
    "        self.linear3 = nn.Linear(300,z_dim)\n",
    "        \n",
    "        \n",
    "        #Img decoding\n",
    "        \n",
    "        self.lin1 = nn.Linear(z_dim, init_kernel*119*119)\n",
    "        '''\n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        '''\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=3, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        \n",
    "        #Polar Coord decoding\n",
    "        self.coord1 = nn.Linear(z_dim, 50)\n",
    "        self.coord2 = nn.Linear(50,10)\n",
    "        self.coord3 = nn.Linear(10,3)\n",
    "    \n",
    "    def forward(self,img,coord):\n",
    "        \n",
    "        img, mu, log_var = ConvVAE.encoder(img)\n",
    "        \n",
    "        #reshape in some way, maybe expansion?\n",
    "        z = torch.cat((img, coord), dim=1) #Maybe 0!!!!!!!!!!\n",
    "        z = self.linear1(z)\n",
    "        z = self.linear2(z)\n",
    "        z = self.linear3(z) #Doesn't have to go back to z_dim I was just feeling it \n",
    "\n",
    "        ##feed new latent vector into deocders that try to get the original things\n",
    "        \n",
    "        #IMG time\n",
    "        #------------------------------------------------\n",
    "        img = self.lin1(z)\n",
    "        img=F.relu(img)\n",
    "        \n",
    "        img=img.view(-1, init_kernel, 119, 119)\n",
    "        #print(\"after unflatten:\")\n",
    "        #print(x.shape)\n",
    "        \n",
    "        #x = self.dec1(x)\n",
    "        #x = F.relu(x)\n",
    "        #x = self.dec2(x)\n",
    "        #x = F.relu(x)\n",
    "        img = self.dec3(img)\n",
    "        img = F.relu(img)\n",
    "        img = self.dec4(img)\n",
    "        img = F.relu(img)\n",
    "        img = self.dec5(img)\n",
    "        reconstruction_img = torch.sigmoid(img)\n",
    "        #------------------------------------------------\n",
    "        \n",
    "        #Coord Time\n",
    "        #------------------------------------------------\n",
    "        coord = self.coord1(z)\n",
    "        coord = F.relu(coord)\n",
    "        coord = self.coord2(coord)\n",
    "        coord = F.relu(coord)\n",
    "        coord = self.coord3(coord)\n",
    "        reconstruction_coord = torch.sigmoid(coord)\n",
    "        \n",
    "        #------------------------------------------------\n",
    "        \n",
    "        \n",
    "        # decode reshaped vector into full image\n",
    "        # decode reshaped vector into polar coords\n",
    "        \n",
    "        return reconstruction_img, reconstruction_coord, z\n",
    "        \n",
    "#How to do loss?\n",
    "# % loss per epoch?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copied straight from original. Need to mess with them a bit to make work for this!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(bce_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_data)/dataloader.batch_size)):\n",
    "        #print(data)\n",
    "        img, _, coord = data\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.to(device)\n",
    "            coord = coord.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #z, mu, logvar = ConvVAE.encoder(img.cuda())\n",
    "        \n",
    "        reconstructed_img, reconstructed_coord, z = model(img, coord)\n",
    "        \n",
    "        bce_loss_img = criterion(reconstructed_img, img)\n",
    "        bce_loss_coord = criterion(reconstructed_coord,coord)\n",
    "        loss = bce_loss_img + bce_loss_coord #final_loss(bce_loss, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            img, _, coord = data\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.to(device)\n",
    "                coord = coord.to(device)\n",
    "            #z, mu, logvar = ConvVAE.encoder(img.cuda())\n",
    "        \n",
    "            reconstructed_img, reconstructed_coord, z = model(img, coord)\n",
    "        \n",
    "            bce_loss_img = criterion(reconstructed_img, img)\n",
    "            bce_loss_coord = criterion(reconstructed_coord,coord)\n",
    "            loss = bce_loss_img + bce_loss_coord #final_loss(bce_loss, mu, logvar)\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_data)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((img.view(batch_size, 3, 128, 128)[:8], \n",
    "                                  reconstructed_img.view(batch_size, 3, 128, 128)[:8]))\n",
    "                save_image(both.cpu(), f\"outputs/{parameter}{value}/imgs/output{epoch}.png\", nrow=num_rows)\n",
    "                wr.writerow([coord, reconstructed_coord])\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Parameters for NEW model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50 # 50,100\n",
    "batch_size = 16 #8,16,32,64?\n",
    "lr = 0.001 #0.002, 0.003\n",
    "\n",
    "kernel_size = 4 #3,4,5,6?\n",
    "stride = 1 #1,2,3,4?\n",
    "padding = 0 #0,1,2,3,4?\n",
    "init_kernel = 8 #4,8,16,32 #initial number of filters\n",
    "\n",
    "latent_dim = 96\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CombNet(latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "parameter = 'testMultiOut'\n",
    "value = 2\n",
    "os.makedirs(\"outputs/\"+parameter+str(value), exist_ok=True)\n",
    "os.makedirs(\"outputs/\"+parameter+str(value)+\"/imgs\", exist_ok=True)\n",
    "#parameters = ['layers']\n",
    "#values = ['3,4,5,6,7']\n",
    "\n",
    "#for parameter in parameters:\n",
    "    #for value in values:\n",
    "\n",
    "        #Create a folder here\n",
    "        #os.makedirs(parameter+value, exist_ok=True)\n",
    "\n",
    "f = open(\"outputs/\"+parameter+str(value)+\"/reconstruction.csv\",\"w\")\n",
    "wr = csv.writer(f)\n",
    "wr.writerow([\"Original\", \"Reconstructed\"])\n",
    "value = str(value)\n",
    "\n",
    "latent = []\n",
    "target = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss = fit(model, train_loader)\n",
    "    val_epoch_loss = validate(model, val_loader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "f.close()\n",
    "filepath = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "torch.save(model.state_dict(), filepath)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yticks(np.arange(int(math.floor(min(train_loss) / 100.0)) * 100, max(train_loss)+1, 1000))\n",
    "plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')\n",
    "\n",
    "with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "    wr.writerows(zip(train_loss, val_loss))\n",
    "\n",
    "with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([parameter, value ,train_loss[-1], val_loss[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(range(1,epochs+1), train_loss, label=\"Train Loss\")\n",
    "plt.plot(range(1,epochs+1), val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.yticks(np.arange(int(math.floor(min(train_loss) / 100.0)) * 100, max(train_loss)+1, 1000))\n",
    "plt.savefig('outputs/'+parameter+value+'/loss'+parameter+value+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/'+parameter+value+'/loss'+parameter+value+'.csv','w', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"Train loss\", \"Val loss\"])\n",
    "    wr.writerows(zip(train_loss, val_loss))\n",
    "\n",
    "with open('outputs/lossCompare.csv', 'a+', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([parameter, value ,train_loss[-1], val_loss[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.join(os.getcwd(), \"outputs\", parameter+str(value), parameter+str(value)+\".pth\")\n",
    "\n",
    "TestVAE = CombNet(latent_dim)#.to(device)\n",
    "TestVAE.load_state_dict(torch.load(PATH))\n",
    "TestVAE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_vectors(model, dataloader):\n",
    "    model.eval()\n",
    "    latent = []\n",
    "    target = []\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_data)/dataloader.batch_size)):\n",
    "            data, label, coord = data\n",
    "            #if torch.cuda.is_available():\n",
    "            #    data = data.to(device)\n",
    "            z, mu, logvar = model.encoder(data.cuda())\n",
    "            latent.extend(mu.cpu().detach().numpy())\n",
    "            target.extend(label.numpy())\n",
    "        return latent, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_latent_var, test_target = generate_latent_vectors(TestVAE, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib qt\n",
    "from IPython import display\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = np.array(test_latent_var)\n",
    "target = np.array(test_target)\n",
    "tsne = TSNE(n_components=2, init=\"pca\", random_state=0)\n",
    "\n",
    "X = tsne.fit_transform(latent)\n",
    "\n",
    "data = np.vstack((X.T, target)).T\n",
    "df = pd.DataFrame(data=data, columns=[\"z1\", \"z2\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].astype(str)\n",
    "\n",
    "fig = px.scatter(df, x=\"z1\", y=\"z2\", color=\"label\")\n",
    "\n",
    "pio.write_html(fig, file='outputs/'+parameter+value+'/plot'+parameter+value+'.html', auto_open=True)\n",
    "#pio.write_html(fig, file='raw.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(train_data))\n",
    "#type(train_loader)\n",
    "#print(train_data.shape)\n",
    "\n",
    "for i, (images, labels, coords) in enumerate(train_loader):\n",
    "    print(images.shape)\n",
    "#    print(labels.shape)\n",
    "    #print(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
